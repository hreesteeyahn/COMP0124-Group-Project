{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    Base class for agents in a simulation environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, idx):\n",
    "        \"\"\"\n",
    "        Initializes an agent with a unique identifier.\n",
    "        \n",
    "        Parameters:\n",
    "            idx (int): An identifier unique to each agent.\n",
    "        \"\"\"\n",
    "        self.idx = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyAgent(Agent):\n",
    "  \"\"\"\n",
    "  A subclass of Agent that implements a greedy strategy for decision-making. \n",
    "  This agent decides on actions based on immediate rewards, with some exploration.\n",
    "  \"\"\"\n",
    "  def __init__(self, idx, discount_rate=0.9, exploration_rate=0.1):\n",
    "    \"\"\"\n",
    "    Initializes a GreedyAgent with specific rates for discount and exploration.\n",
    "    \n",
    "    Parameters:\n",
    "        idx (int): An identifier unique to each agent.\n",
    "        discount_rate (float): Rate at which future rewards are discounted back to their present value.\n",
    "        exploration_rate (float): Probability of choosing a random action to explore the environment.\n",
    "    \"\"\"\n",
    "    super().__init__(idx)\n",
    "    self.possible_actions = {0:'collects apples',\n",
    "                             1:'fires beam',\n",
    "                             2:'walks right',\n",
    "                             3:'walks left',\n",
    "                             4:'walks down',\n",
    "                             5:'walks up'}\n",
    "    self.score = 0\n",
    "    self.discount_rate = discount_rate\n",
    "    self.exploration_rate = exploration_rate\n",
    "\n",
    "  def step(self, state):\n",
    "    \"\"\"\n",
    "    Decides the next action for the agent based on the current state of the environment.\n",
    "    \n",
    "    Parameters:\n",
    "        state (dict): Contains the current state of the environment and necessary information for decision-making.\n",
    "    \n",
    "    Returns:\n",
    "        int: The chosen action as an integer.\n",
    "    \"\"\"\n",
    "    # print(f'Agent {self.idx} is looking around.')\n",
    "    # Extract the agent's current view from the environment state.\n",
    "    self.view = state['agent_views'][self.idx]\n",
    "    # Determine the agent's current position within its view.\n",
    "    pos = tuple(np.argwhere(self.view == str(self.idx))[0])\n",
    "\n",
    "    # Calculate the number of apples and waste within the agent's view.\n",
    "    apples = np.sum(self.view == 'A')\n",
    "    # print(f'Agent {self.idx} can see {apples} apple(s).')\n",
    "    waste = np.sum(self.view[pos[0],:pos[1]] == '#')\n",
    "    # print(f'Agent {self.idx} can clean {waste} waste.')\n",
    "    \n",
    "    # EVALUATE ACTIONS\n",
    "    view_h, view_w = self.view.shape\n",
    "    \n",
    "    mask = np.zeros_like(self.view, dtype=float)\n",
    "    # Compute mask values based on Manhattan distance from the agent's position.\n",
    "    for i in range(view_h):\n",
    "        for j in range(view_w):\n",
    "            distance_x = abs(i - pos[0])\n",
    "            distance_y = abs(j - pos[1])\n",
    "            if   distance_x <= 1 and distance_y <= 1:\n",
    "                mask[i, j] = 1.0\n",
    "            elif distance_x <= 2 and distance_y <= 2 and (distance_x + distance_y) <= 3:\n",
    "                mask[i, j] = 0.7\n",
    "            else:\n",
    "                mask[i, j] = 0.5\n",
    "    masked_view = (self.view=='A') * mask\n",
    "\n",
    "    # Calculate the potential of each action based on the masked view.\n",
    "    action_potential = {}\n",
    "    action_potential[0] = np.sum(masked_view[np.clip(pos[0]-1,0,view_h):np.clip(pos[0]+2,0,view_h),np.clip(pos[1]-1,0,view_w):np.clip(pos[1]+2,0,view_w)])\n",
    "    action_potential[2] = np.sum(masked_view[np.clip(pos[0]-1,0,view_h):np.clip(pos[0]+2,0,view_h),np.clip(pos[1]-0,0,view_w):np.clip(pos[1]+3,0,view_w)])\n",
    "    action_potential[3] = np.sum(masked_view[np.clip(pos[0]-1,0,view_h):np.clip(pos[0]+2,0,view_h),np.clip(pos[1]-2,0,view_w):np.clip(pos[1]+1,0,view_w)])\n",
    "    action_potential[4] = np.sum(masked_view[np.clip(pos[0]-0,0,view_h):np.clip(pos[0]+3,0,view_h),np.clip(pos[1]-1,0,view_w):np.clip(pos[1]+2,0,view_w)])\n",
    "    action_potential[5] = np.sum(masked_view[np.clip(pos[0]-2,0,view_h):np.clip(pos[0]+1,0,view_h),np.clip(pos[1]-1,0,view_w):np.clip(pos[1]+2,0,view_w)])\n",
    "\n",
    "    # POLICY\n",
    "    while True:\n",
    "      # CHOOSE ACTION\n",
    "      if np.random.rand() < self.exploration_rate or action_potential[0] == 0:\n",
    "        # Exploration: choose a random action\n",
    "        action = np.random.randint(1,6)\n",
    "      else:\n",
    "        action = max(action_potential, key=action_potential.get)\n",
    "\n",
    "      # CHECK IF ACTION IS VALID, IF NOT TRY DIFFERENT ACTION\n",
    "      if   action == 0 or action == 1:\n",
    "        break\n",
    "      elif action == 2:\n",
    "        if pos[1] + 1 < view_w and self.view[pos[0], pos[1] + 1] == '_': \n",
    "            break\n",
    "        else:\n",
    "          action_potential[2] = 0.0\n",
    "      elif action == 3:\n",
    "        if pos[1] - 1 > 0 and self.view[pos[0], pos[1] - 1] == '_': \n",
    "            break\n",
    "        else:\n",
    "          action_potential[3] = 0.0\n",
    "      elif action == 4:\n",
    "        if pos[0] + 1 < view_h and self.view[pos[0] + 1, pos[1]] == '_': \n",
    "            break\n",
    "        else:\n",
    "          action_potential[4] = 0.0\n",
    "      elif action == 5:\n",
    "        if pos[0] - 1 > 0 and self.view[pos[0] - 1, pos[1]] == '_': \n",
    "          break\n",
    "        else:\n",
    "          action_potential[5] = 0.0\n",
    "    \n",
    "    # print(f'Agent {self.idx} {self.possible_actions[action]}.')\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillAgent(Agent):\n",
    "  \"\"\"\n",
    "  An agent that adjusts its behavior based on learned skills and the environment's current state. It inherits from the Agent class.\n",
    "\n",
    "  Attributes:\n",
    "      discount_rate (float): Discount rate for future rewards, affecting how future values are discounted into the present value.\n",
    "      exploration_rate (float): Rate at which the agent explores new actions versus exploiting known actions.\n",
    "      possible_actions (dict): Descriptions of possible actions the agent can take.\n",
    "      score (int): Current score of the agent, reflecting its performance.\n",
    "  \"\"\"\n",
    "  def __init__(self, idx, discount_rate=0.9, exploration_rate=0.1, skill_bool=True):\n",
    "    \"\"\"\n",
    "    Initializes a SkillAgent with specific parameters.\n",
    "\n",
    "    Parameters:\n",
    "        idx (int): Unique identifier for the agent.\n",
    "        discount_rate (float): Discount factor for future rewards.\n",
    "        exploration_rate (float): Probability of choosing a random action to explore the environment.\n",
    "    \"\"\"\n",
    "    super().__init__(idx)\n",
    "    self.possible_actions = {0:'collects apples',\n",
    "                             1:'fires beam',\n",
    "                             2:'walks right',\n",
    "                             3:'walks left',\n",
    "                             4:'walks down',\n",
    "                             5:'walks up'}\n",
    "    self.score = 0\n",
    "    self.discount_rate = discount_rate\n",
    "    self.exploration_rate = exploration_rate\n",
    "    self.skill_bool = skill_bool\n",
    "\n",
    "  def set_skill_factor(self, state):\n",
    "    \"\"\"\n",
    "    Determines the agent's skill level based on recent actions.\n",
    "\n",
    "    Parameters:\n",
    "        state (dict): The current state of the environment, including the agent's action history.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: The last skill used and the score associated with that skill.\n",
    "    \"\"\"\n",
    "    # Filter actions related to skills\n",
    "    skill_seq = [a for a in state['action_history'][self] if a in [0,1]]\n",
    "    # Initialize skill score\n",
    "    skill_score = 0\n",
    "    # Determine the last skill used\n",
    "    last_skill = skill_seq[-1] if len(skill_seq) > 0 else None\n",
    "    # Calculate skill score based on consecutive uses of the last skill\n",
    "    while len(skill_seq) > 0:\n",
    "      if last_skill == skill_seq[-1]:\n",
    "        # Increment skill score for repeated use\n",
    "        skill_score += 0.5 # TO ADD: How to define skill\n",
    "        # Remove the last action from consideration\n",
    "        skill_seq = skill_seq[:-1]\n",
    "      else:\n",
    "        break\n",
    "    return last_skill, skill_score\n",
    "  \n",
    "  def calculate_apple_potential(self, pos):\n",
    "    \"\"\"\n",
    "    Calculates the potential for collecting apples based on the agent's position.\n",
    "\n",
    "    Parameters:\n",
    "        pos (tuple): The agent's current position.\n",
    "\n",
    "    Returns:\n",
    "        apple_view (numpy.ndarray): A mask representing the apple collection potential from the current position.\n",
    "    \"\"\"\n",
    "    view_h, view_w = self.view.shape\n",
    "\n",
    "    apple_mask = np.zeros_like(self.view, dtype=float)\n",
    "    # Create a mask based on Manhattan distances to determine apple collection potential\n",
    "    for i in range(view_h):\n",
    "      for j in range(view_w):\n",
    "        distance_x = abs(j - pos[1])\n",
    "        distance_y = abs(i - pos[0])\n",
    "        if   distance_x <= 1 and distance_y <= 1:\n",
    "          apple_mask[i, j] = 1.0\n",
    "        elif distance_x <= 2 and distance_y <= 2 and (distance_x + distance_y) <= 3:\n",
    "          apple_mask[i, j] = 0.7\n",
    "        elif distance_x <= 3 and distance_y <= 3 and (distance_x + distance_y) <= 5:\n",
    "          apple_mask[i, j] = 0.4\n",
    "        else:\n",
    "          apple_mask[i, j] = 0.1\n",
    "    apple_view = (self.view=='A') * apple_mask\n",
    "    return apple_view\n",
    "\n",
    "  def calculate_waste_potential(self, pos):\n",
    "    \"\"\"\n",
    "    Calculates the potential for cleaning waste based on the agent's position.\n",
    "\n",
    "    Parameters:\n",
    "        pos (tuple): The agent's current position.\n",
    "\n",
    "    Returns:\n",
    "        waste_view (numpy.ndarray): A mask representing the waste cleaning potential from the current position.\n",
    "    \"\"\"\n",
    "    view_h = self.view.shape[0]\n",
    "    view_w = self.view.shape[1]    \n",
    "    waste_mask = np.zeros_like(self.view, dtype=float)\n",
    "    # Similar to calculate_apple_potential but focuses on cleaning waste\n",
    "    for i in range(view_h):\n",
    "      for j in range(view_w):\n",
    "        distance_x = abs(j - pos[1])\n",
    "        distance_y = abs(i - pos[0])\n",
    "        if   distance_y <= 0:\n",
    "          waste_mask[i, j] = 0.8\n",
    "        elif distance_y <= 1:\n",
    "          waste_mask[i, j] = 0.5\n",
    "        elif distance_x <= 2:\n",
    "          waste_mask[i, j] = 0.3\n",
    "        else:\n",
    "          waste_mask[i, j] = 0.1\n",
    "    waste_view = (self.view=='#') * waste_mask\n",
    "    # print(waste_view)\n",
    "    return waste_view\n",
    "\n",
    "  def step(self, state):\n",
    "    \"\"\"\n",
    "    Decides on the next action for the agent based on its skills and the current state.\n",
    "\n",
    "    Parameters:\n",
    "        state (dict): The current state of the environment.\n",
    "\n",
    "    Returns:\n",
    "        action (int): The chosen action.\n",
    "    \"\"\"\n",
    "    # print(f'Agent {self.idx} is looking around.')\n",
    "    # Current view of the agent\n",
    "    self.view = state['agent_views'][self.idx]\n",
    "    # Position of the agent\n",
    "    pos = tuple(np.argwhere(self.view == str(self.idx))[0])\n",
    "\n",
    "    # apples = np.sum(self.view == 'A')\n",
    "    # print(f'Agent {self.idx} can see {apples} apple(s).')\n",
    "    # waste = np.sum(self.view[pos[0],:pos[1]] == '#')\n",
    "    # print(f'Agent {self.idx} can clean {waste} waste.')\n",
    "    \n",
    "    # EVALUATE SKILL\n",
    "    # Initialize skill array\n",
    "    skill = np.ones(2)\n",
    "    if self.skill_bool == True:\n",
    "      last_skill, skill_score = self.set_skill_factor(state)\n",
    "      # Adjust skill score based on last action\n",
    "      if last_skill is not None:\n",
    "        skill[last_skill] = skill_score\n",
    "\n",
    "\n",
    "    # EVALUATE ACTIONS\n",
    "    view_h, view_w = self.view.shape\n",
    "    # Dictionary to store potential for each action\n",
    "    action_potential = {}\n",
    "\n",
    "    # Calculate action potentials for different actions based on skills and environmental data\n",
    "    action_potential[0] = skill[0] * np.sum(self.calculate_apple_potential(pos)) + skill[1] * 0.0\n",
    "    action_potential[1] = skill[0] * 0.0 + skill[1] * np.sum(self.calculate_waste_potential(pos))\n",
    "    action_potential[2] = skill[0] * np.sum(self.calculate_apple_potential((pos[0],np.clip(pos[1]+1,0,view_w-1)))[:,np.clip(1,0,view_w-1):]) + skill[1] * np.sum(self.calculate_waste_potential((pos[0],np.clip(pos[1]+1,0,view_w-1)))[:,np.clip(1,0,view_w-1):])\n",
    "    action_potential[3] = skill[0] * np.sum(self.calculate_apple_potential((pos[0],np.clip(pos[1]-1,0,view_w)))[:,:np.clip(view_w-1,0,view_w):]) + skill[1] * np.sum(self.calculate_waste_potential((pos[0],np.clip(pos[1]-1,0,view_w)))[:,:np.clip(view_w-1,0,view_w):])\n",
    "    action_potential[4] = skill[0] * np.sum(self.calculate_apple_potential((np.clip(pos[0]+1,0,view_h-1),pos[1]))[np.clip(1,0,view_h-1):,:]) + skill[1] * np.sum(self.calculate_waste_potential((np.clip(pos[0]+1,0,view_h-1),pos[1]))[np.clip(1,0,view_h-1):,:])\n",
    "    action_potential[5] = skill[0] * np.sum(self.calculate_apple_potential((np.clip(pos[0]-1,0,view_h),pos[1]))[np.clip(view_h-1,0,view_h):,:]) + skill[1] * np.sum(self.calculate_waste_potential((np.clip(pos[0]-1,0,view_h),pos[1]))[:np.clip(view_h-1,0,view_h),:])\n",
    "    \n",
    "\n",
    "    # POLICY\n",
    "    while True:\n",
    "      # CHOOSE ACTION\n",
    "      if np.random.rand() < self.exploration_rate or (action_potential[0] == 0.0 and action_potential[1] == 0.0):\n",
    "        # Exploration\n",
    "        action = np.random.randint(2,6)\n",
    "      else:\n",
    "        # Exploitation\n",
    "        action = max(action_potential, key=action_potential.get)\n",
    "\n",
    "      # CHECK IF ACTION IS VALID, IF NOT TRY DIFFERENT ACTION\n",
    "      if   action == 0 or action == 1:\n",
    "        break\n",
    "      \n",
    "      elif action == 2:\n",
    "        if pos[1] < view_w - 1 and self.view[pos[0], pos[1] + 1] == '_': \n",
    "          break\n",
    "        else:\n",
    "          action_potential[2] = 0.0\n",
    "      elif action == 3:\n",
    "        if pos[1] > 0 and self.view[pos[0], pos[1] - 1] == '_': \n",
    "          break\n",
    "        else:\n",
    "          action_potential[3] = 0.0\n",
    "      elif action == 4:\n",
    "        if pos[0] < view_h - 1 and self.view[pos[0] + 1, pos[1]] == '_': \n",
    "          break\n",
    "        else:\n",
    "          action_potential[4] = 0.0\n",
    "      elif action == 5:\n",
    "        if pos[0] > 0 and self.view[pos[0] - 1, pos[1]] == '_': \n",
    "          break\n",
    "        else:\n",
    "          action_potential[5] = 0.0\n",
    "    \n",
    "    # print(f'Agent {self.idx} {self.possible_actions[action]}.')\n",
    "    return action\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicySkillAgent(Agent):\n",
    "  def __init__(self, idx, discount_rate=0.5, exploration_rate=0.1, skill_bool=True):\n",
    "    super().__init__(idx)\n",
    "    self.possible_actions = {0:'collects apples',\n",
    "                             1:'fires beam',\n",
    "                             2:'walks right',\n",
    "                             3:'walks left',\n",
    "                             4:'walks down',\n",
    "                             5:'walks up'}\n",
    "    self.score = 0\n",
    "    self.discount_rate = discount_rate\n",
    "    self.exploration_rate = exploration_rate\n",
    "    self.skill_bool = skill_bool\n",
    "    self.value_function = np.zeros(len(self.possible_actions))\n",
    "    self.policy = np.zeros(len(self.possible_actions), dtype=int)\n",
    "\n",
    "  def policy_evaluation(self, view, threshold=0.5):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(len(self.possible_actions)):\n",
    "            v = self.value_function[s].copy()\n",
    "            # Calculate the expected return\n",
    "            self.value_function[s] = sum([self.calculate_transition_prob(view, action) * \n",
    "                                          (self.reward(view, action) + self.discount_rate * self.value_function[s])\n",
    "                                          for action in range(len(self.possible_actions))])\n",
    "            delta = max(delta, abs(v - self.value_function[s]))\n",
    "            # print(delta)\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "  def policy_improvement(self, view):\n",
    "    policy_stable = True\n",
    "    for s in range(len(self.possible_actions)):\n",
    "        old_action = self.policy[s]\n",
    "        # Greedily update the policy\n",
    "        self.policy[s] = np.argmax([sum([self.calculate_transition_prob(view, action) * \n",
    "                                        (self.reward(view, action) + self.discount_rate * self.value_function[s])\n",
    "                                        for action in range(len(self.possible_actions))]) for action in range(len(self.possible_actions))])\n",
    "        if old_action != self.policy[s]:\n",
    "            policy_stable = False\n",
    "    return policy_stable\n",
    "\n",
    "  def set_skill_factor(self, state):\n",
    "    \"\"\"\n",
    "    Determines the agent's skill level based on recent actions.\n",
    "\n",
    "    Parameters:\n",
    "        state (dict): The current state of the environment, including the agent's action history.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: The last skill used and the score associated with that skill.\n",
    "    \"\"\"\n",
    "    # Filter actions related to skills\n",
    "    skill_seq = [a for a in state['action_history'][self] if a in [0,1]]\n",
    "    # Initialize skill score\n",
    "    skill_score = 0\n",
    "    # Determine the last skill used\n",
    "    last_skill = skill_seq[-1] if len(skill_seq) > 0 else None\n",
    "    # Calculate skill score based on consecutive uses of the last skill\n",
    "    while len(skill_seq) > 0:\n",
    "      if last_skill == skill_seq[-1]:\n",
    "        # Increment skill score for repeated use\n",
    "        skill_score += 0.5 # TO ADD: How to define skill\n",
    "        # Remove the last action from consideration\n",
    "        skill_seq = skill_seq[:-1]\n",
    "      else:\n",
    "        break\n",
    "    return last_skill, skill_score\n",
    "  \n",
    "  def calculate_apple_potential(self, pos):\n",
    "    \"\"\"\n",
    "    Calculates the potential for collecting apples based on the agent's position.\n",
    "\n",
    "    Parameters:\n",
    "        pos (tuple): The agent's current position.\n",
    "\n",
    "    Returns:\n",
    "        apple_view (numpy.ndarray): A mask representing the apple collection potential from the current position.\n",
    "    \"\"\"\n",
    "    view_h, view_w = self.view.shape\n",
    "\n",
    "    apple_mask = np.zeros_like(self.view, dtype=float)\n",
    "    # Create a mask based on Manhattan distances to determine apple collection potential\n",
    "    for i in range(view_h):\n",
    "      for j in range(view_w):\n",
    "        distance_x = abs(j - pos[1])\n",
    "        distance_y = abs(i - pos[0])\n",
    "        if   distance_x <= 1 and distance_y <= 1:\n",
    "          apple_mask[i, j] = 1.0\n",
    "        elif distance_x <= 2 and distance_y <= 2 and (distance_x + distance_y) <= 3:\n",
    "          apple_mask[i, j] = 0.7\n",
    "        elif distance_x <= 3 and distance_y <= 3 and (distance_x + distance_y) <= 5:\n",
    "          apple_mask[i, j] = 0.4\n",
    "        else:\n",
    "          apple_mask[i, j] = 0.1\n",
    "    apple_view = (self.view=='A') * apple_mask\n",
    "    return apple_view\n",
    "\n",
    "  def calculate_waste_potential(self, pos):\n",
    "    \"\"\"\n",
    "    Calculates the potential for cleaning waste based on the agent's position.\n",
    "\n",
    "    Parameters:\n",
    "        pos (tuple): The agent's current position.\n",
    "\n",
    "    Returns:\n",
    "        waste_view (numpy.ndarray): A mask representing the waste cleaning potential from the current position.\n",
    "    \"\"\"\n",
    "    view_h = self.view.shape[0]\n",
    "    view_w = self.view.shape[1]    \n",
    "    waste_mask = np.zeros_like(self.view, dtype=float)\n",
    "    # Similar to calculate_apple_potential but focuses on cleaning waste\n",
    "    for i in range(view_h):\n",
    "      for j in range(view_w):\n",
    "        distance_x = abs(j - pos[1])\n",
    "        distance_y = abs(i - pos[0])\n",
    "        if   distance_y <= 0:\n",
    "          waste_mask[i, j] = 0.8\n",
    "        elif distance_y <= 1:\n",
    "          waste_mask[i, j] = 0.5\n",
    "        elif distance_x <= 2:\n",
    "          waste_mask[i, j] = 0.3\n",
    "        else:\n",
    "          waste_mask[i, j] = 0.1\n",
    "    waste_view = (self.view=='#') * waste_mask\n",
    "    # print(waste_view)\n",
    "    return waste_view\n",
    "\n",
    "  def calculate_transition_prob(self, view, action):\n",
    "    view_h, view_w = view.shape\n",
    "    pos = tuple(np.argwhere(self.view == str(self.idx))[0])\n",
    "    # CHECK IF ACTION IS VALID, IF NOT TRY DIFFERENT ACTION\n",
    "    if   action == 0 or action == 1:\n",
    "      return 0.1\n",
    "  \n",
    "    elif action == 2:\n",
    "      if pos[1] < view_w - 1 and self.view[pos[0], pos[1] + 1] == '_': \n",
    "        return 0.1\n",
    "      else:\n",
    "        return 0\n",
    "    elif action == 3:\n",
    "      if pos[1] > 0 and self.view[pos[0], pos[1] - 1] == '_': \n",
    "        return 0.1\n",
    "      else:\n",
    "        return 0\n",
    "    elif action == 4:\n",
    "      if pos[0] < view_h - 1 and self.view[pos[0] + 1, pos[1]] == '_': \n",
    "        return 0.1\n",
    "      else:\n",
    "        return 0\n",
    "    elif action == 5:\n",
    "      if pos[0] > 0 and self.view[pos[0] - 1, pos[1]] == '_': \n",
    "        return 0.1\n",
    "      else:\n",
    "        return 0\n",
    "\n",
    "  def reward(self, view, action, skill=[1,1]):\n",
    "      # Calculate the reward for taking action a from state s\n",
    "      view_h, view_w = view.shape\n",
    "      pos = tuple(np.argwhere(self.view == str(self.idx))[0])\n",
    "      \n",
    "      current_potential = skill[0] * np.sum(self.calculate_apple_potential(pos)) + skill[1] * np.sum(self.calculate_waste_potential(pos))\n",
    "\n",
    "      action_potential = {}\n",
    "\n",
    "      action_potential[0] = skill[0] * np.sum(self.calculate_apple_potential(pos)) + skill[1] * 0.0\n",
    "      action_potential[1] = skill[0] * 0.0 + skill[1] * np.sum(self.calculate_waste_potential(pos))\n",
    "      action_potential[2] = skill[0] * np.sum(self.calculate_apple_potential((pos[0],np.clip(pos[1]+1,0,view_w-1)))[:,np.clip(1,0,view_w-1):]) + skill[1] * np.sum(self.calculate_waste_potential((pos[0],np.clip(pos[1]+1,0,view_w-1)))[:,np.clip(1,0,view_w-1):])\n",
    "      action_potential[3] = skill[0] * np.sum(self.calculate_apple_potential((pos[0],np.clip(pos[1]-1,0,view_w)))[:,:np.clip(view_w-1,0,view_w):]) + skill[1] * np.sum(self.calculate_waste_potential((pos[0],np.clip(pos[1]-1,0,view_w)))[:,:np.clip(view_w-1,0,view_w):])\n",
    "      action_potential[4] = skill[0] * np.sum(self.calculate_apple_potential((np.clip(pos[0]+1,0,view_h-1),pos[1]))[np.clip(1,0,view_h-1):,:]) + skill[1] * np.sum(self.calculate_waste_potential((np.clip(pos[0]+1,0,view_h-1),pos[1]))[np.clip(1,0,view_h-1):,:])\n",
    "      action_potential[5] = skill[0] * np.sum(self.calculate_apple_potential((np.clip(pos[0]-1,0,view_h),pos[1]))[np.clip(view_h-1,0,view_h):,:]) + skill[1] * np.sum(self.calculate_waste_potential((np.clip(pos[0]-1,0,view_h),pos[1]))[:np.clip(view_h-1,0,view_h),:])\n",
    "\n",
    "      new_potential = action_potential[action]\n",
    "\n",
    "      reward = new_potential - current_potential\n",
    "      # print(f'Reward {reward}')\n",
    "      return reward\n",
    " \n",
    "  def current_state(self):\n",
    "    # Define how to quantify the current state\n",
    "    pos = tuple(np.argwhere(self.view == str(self.idx))[0])\n",
    "    current_state = self.calculate_apple_potential(pos) + self.calculate_waste_potential(pos)\n",
    "    return current_state\n",
    "\n",
    "  def step(self, state):\n",
    "    self.view = state['agent_views'][self.idx]\n",
    "    # print(self.view)\n",
    "    # Position of the agent\n",
    "\n",
    "    # Policy iteration main loop\n",
    "    self.policy_evaluation(self.view)\n",
    "    if not self.policy_improvement(self.view):\n",
    "      # Choose action based on improved policy\n",
    "      action = self.policy[self.current_state(self.view)]\n",
    "    else:  # or any other exploration strategy\n",
    "      while True:\n",
    "        action = np.random.choice(len(self.possible_actions))\n",
    "        \n",
    "        view_h, view_w = self.view.shape\n",
    "        pos = tuple(np.argwhere(self.view == str(self.idx))[0])\n",
    "        # CHECK IF ACTION IS VALID, IF NOT TRY DIFFERENT ACTION\n",
    "        if   action == 0 or action == 1:\n",
    "          break\n",
    "      \n",
    "        elif action == 2:\n",
    "          if pos[1] < view_w - 1 and self.view[pos[0], pos[1] + 1] == '_': \n",
    "            break\n",
    "          else:\n",
    "            continue\n",
    "        elif action == 3:\n",
    "          if pos[1] > 0 and self.view[pos[0], pos[1] - 1] == '_':  \n",
    "            break\n",
    "          else:\n",
    "            continue\n",
    "        elif action == 4:\n",
    "          if pos[0] < view_h - 1 and self.view[pos[0] + 1, pos[1]] == '_':  \n",
    "            break\n",
    "          else:\n",
    "            continue\n",
    "        elif action == 5:\n",
    "          if pos[0] > 0 and self.view[pos[0] - 1, pos[1]] == '_':  \n",
    "            break\n",
    "          else:\n",
    "            continue\n",
    "\n",
    "    return action\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleanup():\n",
    "  \"\"\"\n",
    "  Represents a cleanup game environment where agents interact with a grid to collect apples and clean waste.\n",
    "\n",
    "  Attributes:\n",
    "      agents (list): A list of agents participating in the environment.\n",
    "      state (dict): A dictionary holding the current state of the environment, including agent views and histories.\n",
    "      map_legend (dict): A legend mapping symbols to their meanings in the environment.\n",
    "      view_size (int): The size of the area visible to each agent.\n",
    "      map (numpy.ndarray): The grid representing the game environment.\n",
    "  \"\"\"\n",
    "  def __init__(self, agents=[], view_size=3, num_apples=0, num_waste=16):\n",
    "    \"\"\"\n",
    "    Initializes the Cleanup environment with a set of agents and a specified view size.\n",
    "\n",
    "    Parameters:\n",
    "        agents (list): A list of agents to include in the environment.\n",
    "        view_size (int): The radius of the square area visible to each agent.\n",
    "    \"\"\"\n",
    "    self.agents = agents\n",
    "    self.state = {}\n",
    "    self.map_legend = {'orchard':'_',\n",
    "                       'river':'~',\n",
    "                       'apple':'A',\n",
    "                       'waste':'#'}\n",
    "    self.view_size = view_size\n",
    "    self.create_map(num_apples, num_waste)\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Resets the environment to its initial state, including action, reward, and communication histories.\n",
    "    \"\"\"\n",
    "    # ACTION HISTORY\n",
    "    # print('Resetting action history.')\n",
    "    self.action_history = {agent:[] for agent in self.agents}\n",
    "    self.state['action_history'] = self.action_history.copy()\n",
    "    # REWARD HISTORY\n",
    "    # print('Resetting reward history.')\n",
    "    self.reward_history = {agent:[] for agent in self.agents}\n",
    "    self.state['reward_history'] = self.reward_history.copy()\n",
    "    # MAP HISTORY\n",
    "    # print('Resetting map history.')\n",
    "    self.state['map_history'] = {0:self.map.copy()}\n",
    "    # COMMUNICATION HISTORY\n",
    "    self.comm_history = {agent:[] for agent in self.agents}\n",
    "    self.state['comm_history'] = self.comm_history.copy()\n",
    "    # SCORES\n",
    "    for agent in self.agents:\n",
    "      agent.score = 0\n",
    "      agent.last_skill = None\n",
    "      agent.skill_factor = 0\n",
    "\n",
    "  def generate_apples(self, num_apples=1):\n",
    "    \"\"\"\n",
    "    Randomly generates a specified number of apples in the orchard area of the map.\n",
    "    \"\"\"\n",
    "    # print(f'Generating {num_apples} apple(s) in the orchard.')\n",
    "    apples = 0\n",
    "    while apples < num_apples:\n",
    "      spawn_loc = (random.randint(0, 9), random.randint(4, 9))\n",
    "      if self.map[spawn_loc] == self.map_legend['orchard']:\n",
    "        self.map[spawn_loc] = self.map_legend['apple']\n",
    "        apples += 1\n",
    "\n",
    "  def generate_waste(self, num_waste=1):\n",
    "    \"\"\"\n",
    "    Randomly generates a specified number of waste blocks in the river area of the map.\n",
    "    \"\"\"\n",
    "    # print(f'Generating {num_waste} block(s) of waste in the river.')\n",
    "    waste = 0\n",
    "    while waste < num_waste:\n",
    "      spawn_loc = (random.randint(0, 9), random.randint(0, 3))\n",
    "      if self.map[spawn_loc] == self.map_legend['river']:\n",
    "        self.map[spawn_loc] = self.map_legend['waste']\n",
    "        waste += 1\n",
    "\n",
    "  def generate_players(self):\n",
    "    \"\"\"\n",
    "    Places agents at random locations within the orchard area of the map.\n",
    "    \"\"\"\n",
    "    # print(f'Generating {len(self.agents)} player(s).')\n",
    "    self.agent_positions = []\n",
    "    while len(self.agent_positions) < len(self.agents):\n",
    "      i = random.randint(0, 9)\n",
    "      j = random.randint(4, 9)\n",
    "      if self.map[i,j] == self.map_legend['orchard']:\n",
    "        self.map[i,j] = str(len(self.agent_positions))\n",
    "        self.agent_positions.append((i,j))\n",
    "  \n",
    "  def get_agent_views(self):\n",
    "    \"\"\"\n",
    "    Updates the view for each agent based on their position and the view size.\n",
    "    \"\"\"\n",
    "    agent_views = []\n",
    "    for i, j in self.agent_positions:\n",
    "      agent_views.append(self.map[np.clip(i-self.view_size, 0, 10) : np.clip(i+self.view_size+1, 0, 10),\n",
    "                                  np.clip(j-self.view_size, 0, 10) : np.clip(j+self.view_size+1, 0, 10)])\n",
    "    self.state['agent_views'] = agent_views\n",
    "\n",
    "  def create_map(self, num_apples, num_waste):    \n",
    "    \"\"\"\n",
    "    Initializes the game map with rivers, orchards, apples, and waste.\n",
    "    \"\"\"\n",
    "    # print('Creating map.')\n",
    "    self.map = np.full((10,10), self.map_legend['orchard'], dtype='U1') # set orchard \n",
    "    self.map[:,0:4] = self.map_legend['river'] # set river\n",
    "\n",
    "    self.generate_apples(num_apples)\n",
    "    self.generate_waste(num_waste)\n",
    "    self.generate_players()\n",
    "    self.get_agent_views()\n",
    "    \n",
    "    # print(self.map)\n",
    "    ''' Example map\n",
    "    ['~' '~' '~' '~' '1' '_' '_' 'A' 'A' '_']\n",
    "    ['~' '~' '~' '~' 'A' '_' '_' '_' '_' '_']\n",
    "    ['~' '~' '~' '~' '_' '_' 'A' '0' 'A' '_']\n",
    "    ['#' '~' '#' '~' 'A' '_' '_' '_' '_' '_']\n",
    "    ['~' '~' '#' '~' '_' '_' 'A' '_' 'A' '_']\n",
    "    ['~' '~' '~' '~' 'A' 'A' '_' '_' '_' 'A']\n",
    "    ['~' '~' '~' '~' '_' '_' '_' '_' '_' '_']\n",
    "    ['~' '~' '~' '#' '_' '_' 'A' 'A' '_' 'A']\n",
    "    ['~' '~' '~' '~' '_' '_' '_' '_' '_' '_']\n",
    "    ['~' '~' '~' '~' '_' 'A' 'A' '_' '_' '_']\n",
    "    where:\n",
    "    'orchard':'_',\n",
    "    'river':'~',\n",
    "    'apple':'A',\n",
    "    'waste':'#',\n",
    "    'agent_0':'0',\n",
    "    'agent_1':'1'\n",
    "    '''\n",
    "\n",
    "  def update_map(self, action, agent):\n",
    "    \"\"\"\n",
    "    Update the game map based on the action taken by an agent.\n",
    "\n",
    "    This method handles different actions such as picking apples, clearing waste with a beam,\n",
    "    and moving the agent in the map. Depending on the action, it updates the map accordingly\n",
    "    and adjusts the agent's score or position.\n",
    "\n",
    "    Parameters:\n",
    "    - action (int): The action code the agent decides to take.\n",
    "    - agent (Agent): The agent performing the action.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of apples picked if the action was picking apples, otherwise 0.\n",
    "    \"\"\"\n",
    "    # Retrieve current agent position\n",
    "    x, y = self.agent_positions[agent.idx]\n",
    "    if   action == 0:\n",
    "      # Define area where apples can be picked based on agent's position\n",
    "      picking_area = self.map[np.clip(x-1,0,10):np.clip(x+2,0,10),np.clip(y-1,0,10):np.clip(y+2,0,10)]\n",
    "      # print(picking_area)\n",
    "      # Count and remove apples from the picking area\n",
    "      apples_picked = np.sum(picking_area==self.map_legend['apple'])\n",
    "      # print(f'Agent {agent.idx} has collected {apples_picked} apples(s).')\n",
    "      picking_area[picking_area==self.map_legend['apple']] = self.map_legend['orchard']\n",
    "\n",
    "      # Update agent's score based on apples picked\n",
    "      agent.score += apples_picked\n",
    "      return apples_picked\n",
    "      \n",
    "    elif action == 1:\n",
    "      # Define beam area for clearing waste\n",
    "      beam = self.map[np.clip(x,0,9),np.clip(y-5,0,9):np.clip(y,0,9)]\n",
    "      # Count and clear waste blocks\n",
    "      waste_cleared = np.sum([beam == '#'])\n",
    "      # print(f'Agent {agent.idx}\\'s beam has cleared {waste_cleared} block(s) of waste.')\n",
    "      beam[beam == '#'] = '~'\n",
    "      # TO ADD: punish agents for being hit by beam\n",
    "      return 0\n",
    "    elif action == 2:\n",
    "      self.map[x,y] = '_'\n",
    "      self.map[x,y+1] = str(agent.idx)\n",
    "      self.agent_positions[agent.idx] = tuple((x,y+1))\n",
    "      return 0\n",
    "    elif action == 3:\n",
    "      self.map[x,y] = '_'\n",
    "      self.map[x,y-1] = str(agent.idx)\n",
    "      self.agent_positions[agent.idx] = tuple((x,y-1))\n",
    "      return 0\n",
    "    elif action == 4:\n",
    "      self.map[x,y] = '_'\n",
    "      self.map[x+1,y] = str(agent.idx)\n",
    "      self.agent_positions[agent.idx] = tuple((x+1,y))\n",
    "      return 0\n",
    "    elif action == 5:\n",
    "      self.map[x,y] = '_'\n",
    "      self.map[x-1,y] = str(agent.idx)\n",
    "      self.agent_positions[agent.idx] = tuple((x-1,y))\n",
    "      return 0\n",
    "\n",
    "  def step(self, num_steps=1):\n",
    "    \"\"\"\n",
    "    Perform simulation steps in the game environment.\n",
    "\n",
    "    This method executes a series of actions for each agent and updates the game state accordingly.\n",
    "    It also handles the generation of waste and apples based on the current state.\n",
    "\n",
    "    Parameters:\n",
    "    - num_steps (int): The number of steps to simulate.\n",
    "    \"\"\"\n",
    "    for step in range(num_steps):\n",
    "\n",
    "      # Handle waste and apple generation based on game conditions\n",
    "      total_waste = np.sum(self.map == self.map_legend.get('waste'))\n",
    "      # print('Waste:', total_waste/40)\n",
    "      if total_waste < 16:\n",
    "        if np.random.uniform() < 0.5:\n",
    "          self.generate_waste()\n",
    "          total_waste += 1\n",
    "      \n",
    "      if total_waste < 16:\n",
    "        respawn_rate = (16 - total_waste) / 16 * 0.5\n",
    "        if np.random.uniform() < respawn_rate:\n",
    "          self.generate_apples()\n",
    "\n",
    "\n",
    "      # Process actions for each agent\n",
    "      for agent in self.agents:      \n",
    "        action = agent.step(self.state)\n",
    "        # if action == 1:\n",
    "        #   print(f'Agent {agent.idx} {agent.possible_actions[action]}')  \n",
    "        \n",
    "        self.action_history[agent].append(action)\n",
    "        self.comm_history[agent].append((agent.last_skill, agent.skill_factor))\n",
    "\n",
    "        # print('Updating map.')\n",
    "        \n",
    "        reward = self.update_map(action, agent)\n",
    "        self.reward_history[agent].append(agent.score)\n",
    "\n",
    "        self.get_agent_views()\n",
    "\n",
    "        if 'QAgent' in agent.__class__.__name__:\n",
    "          new_conv_view = agent.get_conv_view()\n",
    "\n",
    "          if agent.conv_view in agent.q_values.keys():\n",
    "              current_q_values = agent.q_values[agent.conv_view].copy()\n",
    "          else:\n",
    "              current_q_values = np.zeros(6)\n",
    "              \n",
    "          current_action_q = current_q_values[action].copy()\n",
    "          new_action_q = (1 - agent.alpha) * current_action_q + agent.alpha * (reward + agent.discount_rate * max(new_conv_view))\n",
    "          current_q_values[action] = new_action_q\n",
    "          agent.q_values.update({tuple(agent.conv_view) : current_q_values})\n",
    "\n",
    "        # Update state history after each step\n",
    "        self.state['map_history'].update({(step+1):self.map.copy()})\n",
    "        \n",
    "      # print(self.map)\n",
    "  \n",
    "  def evaluate(self, delta): \n",
    "    \"\"\"\n",
    "    Calculate the discounted return for each agent.\n",
    "\n",
    "    Parameters:\n",
    "    - delta (float): The discount factor used for calculating the return.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of discounted returns for each agent.\n",
    "    \"\"\" \n",
    "    disc_returns = [np.sum([r * delta ** i for i, r in enumerate(self.reward_history[agent])]) for agent in self.agents]\n",
    "    return disc_returns\n",
    "  \n",
    "  def visualize_map(self, output_name, fps=2):\n",
    "    \"\"\"\n",
    "    Visualize the game map over time and save it as a video file.\n",
    "\n",
    "    This method goes through each step's map state and converts it into a color image.\n",
    "    These images are then compiled into a video showing the progression of the game environment.\n",
    "    \"\"\"\n",
    "    with imageio.get_writer(output_name, fps=fps) as writer:\n",
    "      for m, char_map in self.state['map_history'].items():\n",
    "        color_map = []\n",
    "        # Mapping characters to colors for visualization\n",
    "        char_to_color = {'_':[  0,255,  0], # green\n",
    "                         '~':[  0,  0,255], # blue\n",
    "                         'A':[255,  0,  0], # red\n",
    "                         '#':[139, 69, 19], # brown\n",
    "                         '0':[255,255,  0], # yellow\n",
    "                         '1':[255,  0,255], # magenta\n",
    "                         '2':[  0,255,255], # cyan\n",
    "                         }\n",
    "        for i in range(10):\n",
    "          for j in range(10):\n",
    "            color_map.append(char_to_color[char_map[i,j]])\n",
    "\n",
    "        # Create and append the frame for current step\n",
    "        frame = np.array(color_map, dtype=np.uint8).reshape((10,10,3))\n",
    "        frame = np.repeat(frame, 50, axis=0)\n",
    "        frame = np.repeat(frame, 50, axis=1)\n",
    "        writer.append_data(frame)\n",
    "\n",
    "  def plot_rewards(self):\n",
    "    \"\"\"\n",
    "    Plot the cumulative score (apples picked) of the agents over time.\n",
    "\n",
    "    This method visualizes the reward history of each agent to compare their performance.\n",
    "    \"\"\"\n",
    "    # Plotting\n",
    "    for agent in self.agents:\n",
    "      plt.plot(self.reward_history[agent], label=f'Agent {agent.idx+1}')\n",
    "    plt.plot([sum(values) for values in zip(*self.reward_history.values())], label=f'Total')\n",
    "\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Cumulative score (apples picked)')\n",
    "    plt.title('Schelling diagram for Cleanup game')\n",
    "\n",
    "    # Adding legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 10\n",
    "num_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "agents = [PolicySkillAgent(idx=0, exploration_rate=0.2, skill_bool=False), # yellow\n",
    "          PolicySkillAgent(idx=1, exploration_rate=0.2, skill_bool=False), # magenta\n",
    "        #   SkillAgent(idx=2, exploration_rate=0.2, skill_bool=True), # cyan\n",
    "          ]\n",
    "\n",
    "game = Cleanup(agents, num_apples=0, num_waste=16)\n",
    "game.step(100)\n",
    "\n",
    "game.visualize_map(r'C:\\Users\\hrist\\OneDrive\\Desktop\\policy_noskill_a0w16.mp4', fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "agents = [PolicySkillAgent(idx=0, exploration_rate=0.2, skill_bool=False), # yellow\n",
    "          PolicySkillAgent(idx=1, exploration_rate=0.2, skill_bool=False), # magenta\n",
    "        #   SkillAgent(idx=2, exploration_rate=0.2, skill_bool=True), # cyan\n",
    "          ]\n",
    "\n",
    "game = Cleanup(agents, num_apples=16, num_waste=16)\n",
    "game.step(100)\n",
    "\n",
    "game.visualize_map(r'C:\\Users\\hrist\\OneDrive\\Desktop\\policy_noskill_a16w16.mp4', fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [PolicySkillAgent(idx=0, exploration_rate=0.2, skill_bool=False), # yellow\n",
    "          PolicySkillAgent(idx=1, exploration_rate=0.2, skill_bool=False), # magenta\n",
    "        #   SkillAgent(idx=2, exploration_rate=0.2, skill_bool=True), # cyan\n",
    "          ]\n",
    "\n",
    "skill_mean_total_running_scores = []\n",
    "skill_mean_ind_running_scores = []\n",
    "for g in range(num_iters):\n",
    "  game = Cleanup(agents)\n",
    "  game.step(num_steps)\n",
    "  skill_mean_total_running_scores.append(np.array([sum(v) for v in zip(*game.reward_history.values())]))\n",
    "  skill_mean_ind_running_scores.append(np.array([v for v in zip(*game.reward_history.values())]))\n",
    "\n",
    "# print(np.mean(mean_total_running_scores, axis=0))\n",
    "print(np.mean(skill_mean_total_running_scores, axis=0)[-1])\n",
    "# print(np.mean(mean_ind_running_scores,axis=0).T)\n",
    "\n",
    "for agent in game.agents:\n",
    "  plt.plot(np.mean(skill_mean_ind_running_scores,axis=0).T[agent.idx], label=f'Agent {agent.idx+1}')\n",
    "plt.plot(np.mean(skill_mean_total_running_scores, axis=0), label=f'Total')\n",
    "# Adding labels and title\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Average cumulative score (apples picked)')\n",
    "plt.title(f'Skill Cleanup ({num_iters} iters)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist([x[-1] for x in skill_mean_total_running_scores], bins=30, alpha=0.75, color='blue', edgecolor='black')\n",
    "plt.title('Histogram Example')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
